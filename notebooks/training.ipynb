{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU 1: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for i, device in enumerate(physical_devices):\n",
    "    print(f\"GPU {i}: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'car_make_images/'\n",
    "training_path = path + 'train'\n",
    "testing_path = path + 'test'\n",
    "validation_path = path + 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_generator = ImageDataGenerator(rescale = 1./255,\n",
    "                              rotation_range = 359,\n",
    "                              shear_range = 0.2,\n",
    "                              width_shift_range = 0.2,\n",
    "                              height_shift_range = 0.2,\n",
    "                              zoom_range = 0.2,\n",
    "                              horizontal_flip = True,\n",
    "                              vertical_flip = True,\n",
    "                              preprocessing_function = None)\n",
    "\n",
    "validation_data_generator = ImageDataGenerator(rescale = 1./255)\n",
    "test_data_generator = ImageDataGenerator(rescale = 1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11573 images belonging to 39 classes.\n",
      "Found 2813 images belonging to 39 classes.\n",
      "Found 2871 images belonging to 39 classes.\n"
     ]
    }
   ],
   "source": [
    "size = 200\n",
    "batch_size = 30  \n",
    "num_classes = 39\n",
    "\n",
    "training_generator = training_data_generator.flow_from_directory(training_path,\n",
    "                                                                 target_size = (size, size),\n",
    "                                                                 batch_size = 30,\n",
    "                                                                 class_mode = \"categorical\",\n",
    "                                                                 color_mode = 'grayscale',\n",
    "                                                                 )\n",
    "\n",
    "validation_generator = validation_data_generator.flow_from_directory(validation_path,\n",
    "                                                                     target_size = (size, size),\n",
    "                                                                     batch_size = 1,\n",
    "                                                                     class_mode = \"categorical\",\n",
    "                                                                     color_mode = 'grayscale',\n",
    "                                                                     )\n",
    "\n",
    "test_generator = test_data_generator.flow_from_directory(testing_path,\n",
    "                                                         target_size = (size, size),\n",
    "                                                         batch_size = 1,\n",
    "                                                         class_mode = \"categorical\",\n",
    "                                                         color_mode = 'grayscale',\n",
    "                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(size, size, 1)))\n",
    "\n",
    "    # First Conv Block\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer='he_normal'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Second Conv Block\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Third Conv Block\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Fourth Conv Block\n",
    "    model.add(Conv2D(filters=256, kernel_size=3, padding='same', kernel_initializer='he_normal'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Flatten and Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_initializer='he_normal'))\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=0.001,\n",
    "                decay_steps=10000,\n",
    "                decay_rate=0.9)\n",
    "    opt = Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 200, 200, 32)      320       \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 200, 200, 32)      0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 100, 100, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 100, 100, 64)      18496     \n",
      "                                                                 \n",
      " re_lu_5 (ReLU)              (None, 100, 100, 64)      0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 50, 50, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 50, 50, 128)       73856     \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 50, 50, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 25, 25, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 25, 25, 256)       295168    \n",
      "                                                                 \n",
      " re_lu_7 (ReLU)              (None, 25, 25, 256)       0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 12, 12, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 36864)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 512)               18874880  \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 39)                10023     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 19,404,071\n",
      "Trainable params: 19,404,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(model, train_data, val_data, epochs, save_interval, model_save_path, history_save_path, custom_metrics=None, custom_optimizer=None):\n",
    "    \"\"\"\n",
    "    Train a TensorFlow model and save it along with its history.\n",
    "    \"\"\"\n",
    "    os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "    if custom_optimizer:\n",
    "        optimizer = custom_optimizer\n",
    "    else:\n",
    "        optimizer = 'adam'\n",
    "\n",
    "    if custom_metrics:\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'] + custom_metrics)\n",
    "    else:\n",
    "        model.compile(optimizer=optimizer,\n",
    "                      loss='categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "    # Initialize variables\n",
    "    initial_epoch = 0\n",
    "    temp_history_data = []\n",
    "\n",
    "    # Check if history file exists, if not create it\n",
    "    if not os.path.exists(history_save_path):\n",
    "        with open(history_save_path, 'w', newline='') as csvfile:\n",
    "            csv_writer = csv.writer(csvfile)\n",
    "            columns = ['Epoch', 'Loss', 'Accuracy', 'Val_Loss', 'Val_Accuracy']\n",
    "            if custom_metrics:\n",
    "                for metric in custom_metrics:\n",
    "                    metric_name = metric.__name__\n",
    "                    columns.append(metric_name)\n",
    "                    columns.append(\"Val_\" + metric_name)\n",
    "            csv_writer.writerow(columns)\n",
    "    else:\n",
    "        with open(history_save_path, 'r') as csvfile:\n",
    "            csv_reader = csv.reader(csvfile)\n",
    "            last_row = None\n",
    "            for row in csv_reader:\n",
    "                last_row = row\n",
    "            if last_row:\n",
    "                initial_epoch = int(last_row[0])\n",
    "\n",
    "    latest_model_file = max(glob.glob(f\"{model_save_path}/model_e*.h5\"), default=None, key=os.path.getctime)\n",
    "    if latest_model_file is not None:\n",
    "        print(f\"Resuming from {latest_model_file}\")\n",
    "        model = tf.keras.models.load_model(latest_model_file, custom_objects={metric.__name__: metric for metric in custom_metrics})\n",
    "\n",
    "    for epoch in range(initial_epoch + 1, epochs + initial_epoch + 1):\n",
    "        print(f\"Epoch {epoch}/{epochs + initial_epoch}\")\n",
    "\n",
    "        history = model.fit(train_data, validation_data=val_data)\n",
    "        history_data = [epoch] + [history.history[key][0] for key in history.history]\n",
    "        temp_history_data.append(history_data)\n",
    "\n",
    "        if epoch % save_interval == 0 or epoch == epochs + initial_epoch:\n",
    "            model_file_path = os.path.join(model_save_path, f\"model_e{epoch}.h5\")\n",
    "            model.save(model_file_path)\n",
    "\n",
    "            # Append to CSV at checkpoints\n",
    "            with open(history_save_path, 'a', newline='') as csvfile:\n",
    "                csv_writer = csv.writer(csvfile)\n",
    "                for row in temp_history_data:\n",
    "                    csv_writer.writerow(row)\n",
    "\n",
    "            # Clear temporary history data\n",
    "            temp_history_data.clear()\n",
    "\n",
    "            print(f\"Saved model and history at epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (actual_positives + K.epsilon())\n",
    "    \n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from models\\model_e4.h5\n",
      "Epoch 5/6\n",
      "386/386 [==============================] - 115s 296ms/step - loss: 3.6223 - accuracy: 0.0442 - f1_score: 0.0000e+00 - val_loss: 3.6199 - val_accuracy: 0.0462 - val_f1_score: 0.0000e+00\n",
      "Saved model and history at epoch 5\n",
      "Epoch 6/6\n",
      "287/386 [=====================>........] - ETA: 23s - loss: 3.6203 - accuracy: 0.0443 - f1_score: 0.0000e+00"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train the model and save it\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39m/GPU:0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     train_and_save(\n\u001b[0;32m      4\u001b[0m         model, \n\u001b[0;32m      5\u001b[0m         training_generator, \n\u001b[0;32m      6\u001b[0m         validation_generator, \n\u001b[0;32m      7\u001b[0m         epochs\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m, \n\u001b[0;32m      8\u001b[0m         save_interval\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[0;32m      9\u001b[0m         model_save_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m     10\u001b[0m         history_save_path\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mhistory.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m     11\u001b[0m         custom_metrics\u001b[39m=\u001b[39;49m[f1_score],\n\u001b[0;32m     12\u001b[0m         custom_optimizer\u001b[39m=\u001b[39;49mopt\n\u001b[0;32m     13\u001b[0m     )\n",
      "Cell \u001b[1;32mIn[29], line 53\u001b[0m, in \u001b[0;36mtrain_and_save\u001b[1;34m(model, train_data, val_data, epochs, save_interval, model_save_path, history_save_path, custom_metrics, custom_optimizer)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(initial_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, epochs \u001b[39m+\u001b[39m initial_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m     51\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39minitial_epoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m     history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_data, validation_data\u001b[39m=\u001b[39;49mval_data)\n\u001b[0;32m     54\u001b[0m     history_data \u001b[39m=\u001b[39m [epoch] \u001b[39m+\u001b[39m [history\u001b[39m.\u001b[39mhistory[key][\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m history\u001b[39m.\u001b[39mhistory]\n\u001b[0;32m     55\u001b[0m     temp_history_data\u001b[39m.\u001b[39mappend(history_data)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model and save it\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    train_and_save(\n",
    "        model, \n",
    "        training_generator, \n",
    "        validation_generator, \n",
    "        epochs=2, \n",
    "        save_interval=1, \n",
    "        model_save_path=\"models\", \n",
    "        history_save_path=\"history.csv\", \n",
    "        custom_metrics=[f1_score],\n",
    "        custom_optimizer=opt\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
