{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "# ImageDataGenerator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "\n",
    "# from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "GPU 1: PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "for i, device in enumerate(physical_devices):\n",
    "    print(f\"GPU {i}: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'car_make_images/'\n",
    "training_path = path + 'train'\n",
    "testing_path = path + 'test'\n",
    "validation_path = path + 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializamos los ImageDataGenerator\n",
    "\n",
    "training_data_generator = ImageDataGenerator(rescale = 1./255,\n",
    "                              rotation_range = 359,\n",
    "                              shear_range = 0.2,\n",
    "                              width_shift_range = 0.2,\n",
    "                              height_shift_range = 0.2,\n",
    "                              zoom_range = 0.2,\n",
    "                              horizontal_flip = True,\n",
    "                              vertical_flip = True,\n",
    "                              preprocessing_function = None)\n",
    "\n",
    "validation_data_generator = ImageDataGenerator(rescale = 1./255)  # Removed augmentation\n",
    "test_data_generator = ImageDataGenerator(rescale = 1./255)  # Removed augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11573 images belonging to 39 classes.\n",
      "Found 2813 images belonging to 39 classes.\n",
      "Found 2871 images belonging to 39 classes.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos los ImageDataGenerator\n",
    "\n",
    "# Para poder usar .flow_from_directory(), necesitamos tener las carpetas ordenadas, es decir:\n",
    "\n",
    "# una carpeta de train con una carpeta de imagenes para cada clase\n",
    "# una carpeta de validation con una carpeta de imagenes para cada clase\n",
    "# y una carpeta de test con una carpeta de imagenes para cada clase\n",
    "\n",
    "size = 200\n",
    "batch_size = 30  # For example\n",
    "num_classes = 39  # Change to the number of classes you actually have\n",
    "\n",
    "training_generator = training_data_generator.flow_from_directory(training_path,\n",
    "                                                                 target_size = (size, size),\n",
    "                                                                 batch_size = 30,\n",
    "                                                                 class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = validation_data_generator.flow_from_directory(validation_path,\n",
    "                                                                     target_size = (size, size),\n",
    "                                                                     batch_size = 1,\n",
    "                                                                     class_mode = \"categorical\")\n",
    "\n",
    "test_generator = test_data_generator.flow_from_directory(testing_path,\n",
    "                                                         target_size = (size, size),\n",
    "                                                         batch_size = 1,\n",
    "                                                         class_mode = \"categorical\")\n",
    "\n",
    "# En caso de que la clasificación sea multi-clase, el cambiamos el parámetro class_mode = \"categorical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with tf.device('/GPU:0'):\n",
    "\n",
    "#     model = Sequential()\n",
    "\n",
    "#     model.add(Input(shape = (size, size, 3)))\n",
    "\n",
    "#     model.add(Conv2D(filters = 16, kernel_size = 9, padding = \"same\", activation = \"relu\"))\n",
    "#     model.add(MaxPooling2D(pool_size = 2))\n",
    "\n",
    "#     model.add(Conv2D(filters = 16, kernel_size = 9, padding = \"same\", activation = \"relu\"))\n",
    "#     model.add(MaxPooling2D(pool_size = 2))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "\n",
    "#     model.add(Dense(units = 120, activation = \"relu\"))\n",
    "#     model.add(Dense(units = 120, activation = \"relu\"))\n",
    "#     model.add(Dense(units = 120, activation = \"relu\"))\n",
    "#     model.add(Dropout(0.15))\n",
    "#     model.add(Dense(units = num_classes, activation = \"softmax\"))  # Changed to 'softmax'\n",
    "    \n",
    "#     # Model Compilation\n",
    "#     model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "\n",
    "#     # Model Training\n",
    "#     history = model.fit(training_generator, epochs = 1, validation_data = validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    beta_squared = 4  # beta=2, beta^2 = 4\n",
    "    tp = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f2_val = (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n",
    "    return f2_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(size, num_classes):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(size, size, 3)))\n",
    "\n",
    "    # First Conv Block\n",
    "    model.add(Conv2D(filters=32, kernel_size=3, padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Second Conv Block\n",
    "    model.add(Conv2D(filters=64, kernel_size=3, padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Third Conv Block\n",
    "    model.add(Conv2D(filters=128, kernel_size=3, padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Fourth Conv Block\n",
    "    model.add(Conv2D(filters=256, kernel_size=3, padding='same', kernel_initializer='he_normal'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(tf.keras.layers.ReLU())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "    # Flatten and Fully Connected Layers\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.15))\n",
    "    model.add(Dense(256, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dense(num_classes, activation='softmax', kernel_initializer='he_normal'))\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def train_models(learning_rates, batch_sizes, training_generator, validation_generator):\n",
    "    results = []\n",
    "    \n",
    "    for lr in learning_rates:\n",
    "        for batch in batch_sizes:\n",
    "            \n",
    "            # Define the learning rate schedule\n",
    "            lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                initial_learning_rate=lr,\n",
    "                decay_steps=10000,\n",
    "                decay_rate=0.9)\n",
    "            opt = Adam(learning_rate=lr_schedule)\n",
    "            \n",
    "            # Define early stopping\n",
    "            early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "            \n",
    "            # Define the model (assumes you have a function that returns your model)\n",
    "            model = create_model(size, num_classes)  # Replace this with your actual model function\n",
    "            model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "            \n",
    "            # Train the model\n",
    "            history = model.fit(training_generator, epochs=100, batch_size=batch,\n",
    "                                validation_data=validation_generator, \n",
    "                                callbacks=[early_stopping])\n",
    "            \n",
    "            # Save the model\n",
    "            model_name = f'model_lr_{lr}_batch_{batch}.h5'\n",
    "            model.save(model_name)\n",
    "            \n",
    "            # Save metrics\n",
    "            metrics_df = pd.DataFrame(history.history)\n",
    "            metrics_name = f'metrics_lr_{lr}_batch_{batch}.csv'\n",
    "            metrics_df.to_csv(metrics_name, index=False)\n",
    "            \n",
    "            # Save the number of epochs trained and append to results\n",
    "            n_epochs = len(history.history['accuracy'])\n",
    "            results.append({\n",
    "                'learning_rate': lr,\n",
    "                'batch_size': batch,\n",
    "                'epochs_trained': n_epochs,\n",
    "                'final_val_accuracy': history.history['val_accuracy'][-1]\n",
    "            })\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('training_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "386/386 [==============================] - 137s 352ms/step - loss: 5.4602 - accuracy: 0.0356 - val_loss: 3.6256 - val_accuracy: 0.0398\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 181s 469ms/step - loss: 3.6295 - accuracy: 0.0444 - val_loss: 3.6210 - val_accuracy: 0.0462\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 149s 385ms/step - loss: 3.6213 - accuracy: 0.0440 - val_loss: 3.6191 - val_accuracy: 0.0462\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 166s 431ms/step - loss: 3.6210 - accuracy: 0.0442 - val_loss: 3.6189 - val_accuracy: 0.0462\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 149s 384ms/step - loss: 3.6208 - accuracy: 0.0442 - val_loss: 3.6188 - val_accuracy: 0.0462\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 140s 364ms/step - loss: 3.6210 - accuracy: 0.0442 - val_loss: 3.6189 - val_accuracy: 0.0462\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 148s 383ms/step - loss: 3.6209 - accuracy: 0.0440 - val_loss: 3.6186 - val_accuracy: 0.0462\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 142s 365ms/step - loss: 5.4564 - accuracy: 0.0346 - val_loss: 3.6342 - val_accuracy: 0.0462\n",
      "Epoch 2/100\n",
      "386/386 [==============================] - 153s 397ms/step - loss: 3.6266 - accuracy: 0.0444 - val_loss: 3.6102 - val_accuracy: 0.0501\n",
      "Epoch 3/100\n",
      "386/386 [==============================] - 200s 517ms/step - loss: 3.6195 - accuracy: 0.0450 - val_loss: 3.6192 - val_accuracy: 0.0462\n",
      "Epoch 4/100\n",
      "386/386 [==============================] - 195s 505ms/step - loss: 3.6209 - accuracy: 0.0441 - val_loss: 3.6189 - val_accuracy: 0.0462\n",
      "Epoch 5/100\n",
      "386/386 [==============================] - 184s 476ms/step - loss: 3.6207 - accuracy: 0.0442 - val_loss: 3.6189 - val_accuracy: 0.0462\n",
      "Epoch 6/100\n",
      "386/386 [==============================] - 196s 508ms/step - loss: 3.6223 - accuracy: 0.0442 - val_loss: 3.6189 - val_accuracy: 0.0462\n",
      "Epoch 7/100\n",
      "386/386 [==============================] - 161s 416ms/step - loss: 3.6209 - accuracy: 0.0442 - val_loss: 3.6189 - val_accuracy: 0.0462\n",
      "Epoch 1/100\n",
      "386/386 [==============================] - 155s 400ms/step - loss: 9.3736 - accuracy: 0.0402 - val_loss: 3.6231 - val_accuracy: 0.0462\n",
      "Epoch 2/100\n",
      "322/386 [========================>.....] - ETA: 22s - loss: 3.6328 - accuracy: 0.0429"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m batch_sizes \u001b[39m=\u001b[39m [\u001b[39m32\u001b[39m, \u001b[39m64\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39m/GPU:0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m----> 5\u001b[0m     train_models(learning_rates, batch_sizes, training_generator, validation_generator)\n",
      "Cell \u001b[1;32mIn[49], line 26\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(learning_rates, batch_sizes, training_generator, validation_generator)\u001b[0m\n\u001b[0;32m     23\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mopt, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(training_generator, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch,\n\u001b[0;32m     27\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mvalidation_generator, \n\u001b[0;32m     28\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n\u001b[0;32m     30\u001b[0m \u001b[39m# Save the model\u001b[39;00m\n\u001b[0;32m     31\u001b[0m model_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmodel_lr_\u001b[39m\u001b[39m{\u001b[39;00mlr\u001b[39m}\u001b[39;00m\u001b[39m_batch_\u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m}\u001b[39;00m\u001b[39m.h5\u001b[39m\u001b[39m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\engine\\training.py:1570\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1568\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs\n\u001b[0;32m   1569\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[1;32m-> 1570\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[0;32m   1571\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[0;32m   1572\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\callbacks.py:470\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[0;32m    464\u001b[0m \n\u001b[0;32m    465\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[1;32m--> 470\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m\"\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m\"\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\callbacks.py:317\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[0;32m    316\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[0;32m    318\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    320\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExpected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\callbacks.py:340\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    337\u001b[0m     batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[0;32m    338\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[1;32m--> 340\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[0;32m    342\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[0;32m    343\u001b[0m     end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\callbacks.py:388\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m    387\u001b[0m     hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[1;32m--> 388\u001b[0m     hook(batch, logs)\n\u001b[0;32m    390\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\callbacks.py:1081\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1081\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\callbacks.py:1157\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[0;32m   1155\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1156\u001b[0m     \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m     logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[0;32m   1158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\utils\\tf_utils.py:635\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[39mreturn\u001b[39;00m t\n\u001b[0;32m    633\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(t) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m t\n\u001b[1;32m--> 635\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\util\\nest.py:917\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    913\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[0;32m    914\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[0;32m    916\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[1;32m--> 917\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[0;32m    918\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\keras\\utils\\tf_utils.py:628\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[0;32m    626\u001b[0m     \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m--> 628\u001b[0m         t \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[0;32m    629\u001b[0m     \u001b[39m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[39m# as-is.\u001b[39;00m\n\u001b[0;32m    631\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(t, (np\u001b[39m.\u001b[39mndarray, np\u001b[39m.\u001b[39mgeneric)):\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1157\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1134\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1156\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[1;32m-> 1157\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1158\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[1;32mc:\\Users\\dimit\\OneDrive\\Escritorio\\project_hackaboss\\.env\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1123\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1122\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[0;32m   1124\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1125\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "learning_rates = [0.005, 0.01] # [0.001, 0.005, 0.01]\n",
    "batch_sizes = [32, 64]\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    train_models(learning_rates, batch_sizes, training_generator, validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
